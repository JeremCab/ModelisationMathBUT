{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d9e998",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3336ce",
   "metadata": {},
   "source": [
    "## Classify iris plants into three species\n",
    "\n",
    "Le but de ce projet est d'implémenter un **naive Bayes classifier** (from scratch) qui permet de classifier trois types d'iris (fleurs) en fonction de la taille de leur pétales et sépales.\n",
    "\n",
    "Pour plus de précision sur les data, voir le lien suivant:<br>\n",
    "https://www.kaggle.com/datasets/uciml/iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d670aa3",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d80fb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159601ec",
   "metadata": {},
   "source": [
    "## Loader les data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9c1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa869341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal.length  sepal.width  petal.length  petal.width    variety\n",
       "0             5.1          3.5           1.4          0.2     Setosa\n",
       "1             4.9          3.0           1.4          0.2     Setosa\n",
       "2             4.7          3.2           1.3          0.2     Setosa\n",
       "3             4.6          3.1           1.5          0.2     Setosa\n",
       "4             5.0          3.6           1.4          0.2     Setosa\n",
       "..            ...          ...           ...          ...        ...\n",
       "145           6.7          3.0           5.2          2.3  Virginica\n",
       "146           6.3          2.5           5.0          1.9  Virginica\n",
       "147           6.5          3.0           5.2          2.0  Virginica\n",
       "148           6.2          3.4           5.4          2.3  Virginica\n",
       "149           5.9          3.0           5.1          1.8  Virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e055f",
   "metadata": {},
   "source": [
    "Nous allons prédire la variable `variety` en fonction des variables `sepal.length`, \t`sepal.width`, `petal.length`, `petal.width`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3bd645",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d090cc",
   "metadata": {},
   "source": [
    "Soient $\\mathbf{X} = (X_1 ,\\dots, X_P)$ des *variables explicatives*,  $Y$ une *variable réponse* qualitative à valeurs dans $C = \\{ c_1, \\dots ,c_K \\}$. \n",
    "\n",
    "Soit également le *train set* $S = \\big\\{ (\\mathbf{x_i}, y_i) \\in \\mathbb{R}^P \\times C :  i = 1, \\dots, N \\big\\}$. Pour tout $k =  1, \\dots, K$, on définit $S_k$ comme étant le sous-ensemble du train set $S$ formé des éléments de la classe $c_k$.\n",
    "\n",
    "Un **Naive Bayes classifier** est un modèle probabiliste de classification qui permet d'estimer la probabilité conditionnelle $p(Y = c_k \\mid \\mathbf{X} = \\mathbf{x})$ que le point $\\mathrm{x}$ appartienne à la classe $c_k$, pour tout $k=1,\\dots,K$.\n",
    "\n",
    "En utilisant **hypothèse naïve d’indépendance conditionnelle** ainsi que le **théorème de Bayes**, la *probabilité conditionnelle* et la *prédiction* sont données par les formules suivantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bdef50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "$$\n",
    "p(c_k  \\mid \\mathbf{x}) ~\\propto~ p(c_k) \\prod_{j=1}^{P} p(x_{j} \\mid c_k) ~~~\\text{ et }~~~ \\hat c ~=~ \\arg \\max_{c_k \\in C} \\, p(c_k \\mid \\mathbf{x})\n",
    "$$\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77301c0c",
   "metadata": {},
   "source": [
    "Dans cette formule, les probabilités $p(c_k)$ et $p(x_{j} \\mid c_k)$ doivent être *estimées* à partir des data. Dans ce context, on prendra:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611619d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "$$\n",
    "p(c_k) = \\frac{| S_k |}{N}\n",
    "$$\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416a443",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "$$\n",
    "p(x_j \\mid c_{k}) = \\frac{1}{\\sqrt {2\\pi \\sigma_{jk}^2}} \\,\\exp \\left( -\\frac{(x_j - \\mu_{jk})^2}{2 \\sigma_{jk}^2} \\right)\n",
    "$$\n",
    "où\n",
    "$$\n",
    "\\mu_{jk} =\\frac{1}{| S_k |} \\sum_{(\\mathbf{x}, y) \\in S_k} x_{j} ~~~\\text{ et }~~~\n",
    "\\sigma_{jk}^{2} = \\frac{1}{(| S_k | - 1)} \\sum_{(\\mathbf{x}, y) \\in S_k} \\left(x_{j} - \\mu_{jk} \\right)^{2}\n",
    "$$\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a5510",
   "metadata": {},
   "source": [
    "La classe `NaiveBayesClassifier` implémente un **Naive Bayes classifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe70cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier():\n",
    "    \"\"\"\n",
    "    Implements a Naive Bayes Classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_fts = 0\n",
    "        self.classes = []\n",
    "        self.class_priors = {}\n",
    "        self.mu = None\n",
    "        self.sigma2 = None\n",
    "        \n",
    "    \n",
    "    def initialize(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Update the atributes self.num_fts and self.classes \n",
    "        based on the train set (X_train, y_train).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : ndarray\n",
    "            features of the train set\n",
    "        y_train : ndarray\n",
    "            responses of the train set\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_fts = X_train.shape[1]\n",
    "        try:\n",
    "            y_train = y_train.flatten()\n",
    "        except:\n",
    "            pass\n",
    "        self.classes = np.unique(y_train)\n",
    "    \n",
    "    \n",
    "    def compute_class_priors(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Estimates the class priors $p(c_k)$ from the train set.\n",
    "        \n",
    "        Updates the attribute self.class_priors as follows:\n",
    "        - the keys of the dict are the classes\n",
    "        - the values of the dict are the corrresponding prior probabilities\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : ndarray\n",
    "            features of the train set\n",
    "        y_train : ndarray\n",
    "            responses of the train set\n",
    "        \"\"\"\n",
    "        \n",
    "        nb_elements = X_train.shape[0]\n",
    "        \n",
    "        for c in self.classes:\n",
    "            \n",
    "            prior = np.sum(y_train==c) / nb_elements\n",
    "            self.class_priors[c] = prior\n",
    "    \n",
    "    \n",
    "    def compute_fts_distr_params(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Estimates the parameters of the feature distributions $p(x_j | c_k)$ \n",
    "        from the train set, for each feature $X_j$ and each class $c_k.\n",
    "        \n",
    "        Updates the attributes self.mu and self.sigma2 as follows:\n",
    "        self.mu and self.sigma2 are 2-dim arrays:\n",
    "        - the (j, k)-th element of self.mu is $\\mu_{jk}$\n",
    "        - the (j, k)-th element of self.sigma2 is $\\sigma^2_{jk}$\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : ndarray\n",
    "            features of the train set\n",
    "        y_train : ndarray\n",
    "            responses of the train set\n",
    "        \"\"\"\n",
    "        \n",
    "        num_classes = len(self.classes)\n",
    "        self.mu = np.ones(shape=(self.num_fts, num_classes))\n",
    "        self.sigma2 = np.ones(shape=(self.num_fts, num_classes))\n",
    "        \n",
    "        for k, c in enumerate(self.classes):\n",
    "            \n",
    "            mask = y_train.flatten() == c\n",
    "            X_train_sub = X_train[mask]\n",
    "            nb_elements = np.sum(mask)\n",
    "            \n",
    "            # using numpy functions\n",
    "            self.mu[:, k] = np.mean(X_train_sub, axis=0)\n",
    "            self.sigma2[:, k] = np.var(X_train_sub, axis=0)\n",
    "            \n",
    "            # implmenting the formula\n",
    "            #for j in range(self.num_fts):\n",
    "            #    \n",
    "            #    self.mu[j, k] = np.sum(X_train_sub[:, j]) / nb_elements\n",
    "            #    \n",
    "            #    nb_elements -= 1\n",
    "            #    self.sigma2[j, k] = np.sum((X_train_sub[:, j] - self.mu[j, k])**2) / nb_elements \n",
    "\n",
    "                \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fits the naive Bayes classifier on the train set.\n",
    "        After fitting, the parameters of the classifier are computed, i.e.:\n",
    "        - the class priors\n",
    "        - the means and variances of the features' distributions\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : ndarray\n",
    "            features of the train set\n",
    "        y_train : ndarray\n",
    "            responses of the train set\n",
    "        \"\"\"\n",
    "        \n",
    "        self.initialize(X_train, y_train)\n",
    "        self.compute_class_priors(X_train, y_train)\n",
    "        self.compute_fts_distr_params(X_train, y_train)\n",
    "        \n",
    "    \n",
    "    def Gaussian_density(self, x, mu, sigma2):\n",
    "        \"\"\"\n",
    "        Computes the density of probabilty at x of the Gaussian distribution\n",
    "        of mean and variance mu and sigma2, respectively.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : float\n",
    "        mu : float\n",
    "        sigma2 : float\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        result : float\n",
    "        \"\"\"\n",
    "        \n",
    "        num = np.exp( -((x - mu)**2) / (2*sigma2) )\n",
    "        denom = np.sqrt(2 * np.pi * sigma2)\n",
    "        result = num / denom\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict the class probabilities of a test set X_test.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test : ndarray\n",
    "            Test set\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_proba : ndarray\n",
    "            Tensor of class probabilities for each element of the test sets.\n",
    "        \"\"\"\n",
    "        \n",
    "        nb_elements = len(X_test)\n",
    "        nb_classes = len(self.classes)\n",
    "        y_proba = np.zeros(shape=(nb_elements, nb_classes))\n",
    "        \n",
    "        for e in range(nb_elements):\n",
    "        \n",
    "            x = X_test[e, :]\n",
    "            for k, c in enumerate(self.classes):\n",
    "                \n",
    "                p_ck = self.class_priors[c]\n",
    "                p_xjck = 1.\n",
    "\n",
    "                for j in range(self.num_fts):\n",
    "                    mu = self.mu[j,k]\n",
    "                    sigma2 = self.sigma2[j,k]\n",
    "                    p_xjck *= self.Gaussian_density(x[j], mu, sigma2)\n",
    "\n",
    "                p_ckx = p_ck * p_xjck\n",
    "\n",
    "                y_proba[e, k] = p_ckx\n",
    "            \n",
    "        return y_proba\n",
    "    \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict the classes of a test set X_test.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_test : ndarray\n",
    "            Test set\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : ndarray\n",
    "            Tensor of predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        y_proba = self.predict_proba(X_test)\n",
    "        y_pred = np.argmax(y_proba, axis=1)\n",
    "        y_pred = [nb.classes[i] for i in y_pred]\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf645d7",
   "metadata": {},
   "source": [
    "### Exercice 1\n",
    "\n",
    "Complétez la méthode `initialize(...)` qui, étant donné un train set `(X_train, y_train)` met à jour les attribut `self.num_fts` et `self.num_classes` qui correspondent au nombre de features et au nombre de classes du train set, respectivement.\n",
    "\n",
    "Testez votre méthode comme suit:\n",
    "```\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "nb = NaiveBayesClassifier()\n",
    "nb.initialize(X_train, y_train)\n",
    "print(f\"Number of features: {nb.num_fts}\\nClasses: {nb.classes}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "182eb801",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28624256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 4\n",
      "Classes: ['Setosa' 'Versicolor' 'Virginica']\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayesClassifier()\n",
    "nb.initialize(X_train, y_train)\n",
    "print(f\"Number of features: {nb.num_fts}\\nClasses: {nb.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa48e56",
   "metadata": {},
   "source": [
    "### Exercice 2\n",
    "\n",
    "Complétez la méthode `compute_class_priors(...)` qui, étant donné un train set `(X_train, y_train)` met à jour l'attribut `self.class_priors`.  \n",
    "\n",
    "\n",
    "L'attribut `self.class_priors` est un dictionnaire dont chaque clé est une classe et chaque valeur est la  **probabilités a priori de cette classe (class prior) $p(c_k)$** qui est donnée par la première formule en jaune ci-dessus.\n",
    "\n",
    "Testez votre méthode comme suit:\n",
    "```\n",
    "nb = NaiveBayesClassifier()\n",
    "nb.initialize(X_train, y_train)\n",
    "nb.compute_class_priors(X_train, y_train)\n",
    "nb.class_priors\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "272ae482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Setosa': 0.3333333333333333,\n",
       " 'Versicolor': 0.3416666666666667,\n",
       " 'Virginica': 0.325}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = NaiveBayesClassifier()\n",
    "nb.initialize(X_train, y_train)\n",
    "nb.compute_class_priors(X_train, y_train)\n",
    "nb.class_priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d2baf",
   "metadata": {},
   "source": [
    "### Exercice 3\n",
    "\n",
    "Complétez la méthode `compute_fts_distr_params(...)` qui, étant donné un train set `(X_train, y_train)` met à jour les attributs `self.mu` et `self.sigma2`.\n",
    "\n",
    "- L'attribut `self.mu` est un 2D-array dont le $(j,k)$-ème élément est $\\mu_{jk}$ donné par la seconde formule jaune\n",
    "- L'attribut `self.sigma2` est un 2D-array dont $(j,k)$-élément est $\\sigma^2_{jk}$ donné par la seconde formule jaune\n",
    "\n",
    "Pour vous éviter de réimplémenter la formule de la moyenne et de la variance, vous pouvez utiliser les instructions `np.mean` et `np.var` (avec l'argument `axis = 0`) appliquées au sous-dataset $S_k$ ($k=1,\\dots,3$).\n",
    "\n",
    "Testez votre méthode comme suit:\n",
    "```\n",
    "nb = NaiveBayesClassifier()\n",
    "nb.initialize(X_train, y_train)\n",
    "nb.compute_class_priors(X_train, y_train)\n",
    "nb.compute_fts_distr_params(X_train, y_train)\n",
    "print(f\"Means and variance of the features' distributions:\")\n",
    "nb.mu, nb.sigma2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83a4b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means and variance of the features' distributions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[4.99      , 5.9195122 , 6.53333333],\n",
       "        [3.4525    , 2.77073171, 2.96666667],\n",
       "        [1.45      , 4.24146341, 5.52051282],\n",
       "        [0.245     , 1.32195122, 2.        ]]),\n",
       " array([[0.1239    , 0.28693635, 0.4165812 ],\n",
       "        [0.15249375, 0.10011898, 0.0991453 ],\n",
       "        [0.033     , 0.22584176, 0.28573307],\n",
       "        [0.010975  , 0.04122546, 0.08205128]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = NaiveBayesClassifier()\n",
    "nb.initialize(X_train, y_train)\n",
    "nb.compute_class_priors(X_train, y_train)\n",
    "nb.compute_fts_distr_params(X_train, y_train)\n",
    "print(f\"Means and variance of the features' distributions:\")\n",
    "nb.mu, nb.sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c2f2f",
   "metadata": {},
   "source": [
    "### Exercice 4\n",
    "\n",
    "Complétez la méthode `fit(...)` qui, étant donné un train set `(X_train, y_train)` applique sucessivement les méthodes:\n",
    "1. `initialize(...)`\n",
    "2. `compute_class_priors(...)`\n",
    "3. `compute_fts_distr_params(...)`\n",
    "\n",
    "Une fois ces méthodes exécutées, tous les paramètres $p(c_k)$, $\\mu_{jk}$ et $\\sigma^2_{jk}$ ($j=1,\\dots,P$ et $k=1,\\dots,K$) de notre classifieur sont calculés.\n",
    "\n",
    "\n",
    "Testez votre méthode comme suit:\n",
    "```\n",
    "nb = NaiveBayesClassifier()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Class priors:\\n\\n\", nb.class_priors)\n",
    "print(f\"\\nMeans and variances of the features' distributions:\")\n",
    "nb.mu, nb.sigma2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a00415bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class priors:\n",
      "\n",
      " {'Setosa': 0.3333333333333333, 'Versicolor': 0.3416666666666667, 'Virginica': 0.325}\n",
      "\n",
      "Means and variances of the features' distributions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[4.99      , 5.9195122 , 6.53333333],\n",
       "        [3.4525    , 2.77073171, 2.96666667],\n",
       "        [1.45      , 4.24146341, 5.52051282],\n",
       "        [0.245     , 1.32195122, 2.        ]]),\n",
       " array([[0.1239    , 0.28693635, 0.4165812 ],\n",
       "        [0.15249375, 0.10011898, 0.0991453 ],\n",
       "        [0.033     , 0.22584176, 0.28573307],\n",
       "        [0.010975  , 0.04122546, 0.08205128]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = NaiveBayesClassifier()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Class priors:\\n\\n\", nb.class_priors)\n",
    "print(f\"\\nMeans and variances of the features' distributions:\")\n",
    "nb.mu, nb.sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e469d7a",
   "metadata": {},
   "source": [
    "### Exercice 6\n",
    "\n",
    "Complétez la méthode `compute_Gaussian_density(...)` qui, étant donné un point `x`, une moyenne `mu` est une variance `sigma2` calcule la densité de probabilité Gaussienne suivante:\n",
    "$$\n",
    "f(x) = \n",
    "\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\\; \\exp\\left(-\\frac{\\left( x - \\mu \\right)^2}{2\\sigma^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aea93c",
   "metadata": {},
   "source": [
    "### Exercice 7\n",
    "\n",
    "Complétez la méthode `predict_proba(...)` qui, étant donné un test set `X_test`, calcule les probabilités $p(c_k  \\mid \\mathbf{x})$ ($k = 1, \\dots, K$) selon la formule bleue ci-dessus.\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "y_proba = nb.predict_proba(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23c2e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = nb.predict_proba(X_test)\n",
    "# y_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1cae9",
   "metadata": {},
   "source": [
    "### Exercice 8\n",
    "\n",
    "Complétez la méthode `predict(...)` qui, étant donné un test set `X_test`, calcule les predictions $\\hat c$ selon la formule bleue (de droite) ci-dessus. Pour cela, il vous suffit de prendre l'argmax de tableau `predict_proba(X_test)`.\n",
    "\n",
    "Tester votre méthode comme suit:\n",
    "```\n",
    "y_pred = nb.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cccc1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Setosa       1.00      1.00      1.00        10\n",
      "  Versicolor       1.00      1.00      1.00         9\n",
      "   Virginica       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb.predict(X_test)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
